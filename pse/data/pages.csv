url,title,description,is_pdf,is_twitter
https://github.com/mikepapadim/llama-shepherd-cli,GitHub - mikepapadim/llama-shepherd-cli: A CLI to manage install and configure llama inference implemenation in multiple languages,A CLI to manage install and configure llama inference implemenation in multiple languages - GitHub - mikepapadim/llama-shepherd-cli: A CLI to manage install and configure llama inference implemenat...,False,False
https://people.inf.ethz.ch/wirth/ProjectOberon/PO.System.pdf,,,True,False
https://chrisnicholas.dev/blog/a-new-blog-for-2024,A new blog for 2024 | Chris Nicholas | Developer experience at Liveblocks,DX engineer at Liveblocks. Articles on developer experience and web engineering.,False,False
https://browse.arxiv.org/html/2312.08361v1,Distributed Inference and Fine-tuning of Large Language Models Over The Internet,,False,False
https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/,hackerllama - The Random Transformer,Understand how transformers work by demystifying all the math behind them,False,False
https://github.com/html5-ninja/page-replica,"GitHub - html5-ninja/page-replica: Page Replica ‚Äì Tool for Web Scraping, Prerendering, and SEO Boost","Page Replica ‚Äì Tool for Web Scraping, Prerendering, and SEO Boost - GitHub - html5-ninja/page-replica: Page Replica ‚Äì Tool for Web Scraping, Prerendering, and SEO Boost",False,False
https://thisisimportant.net/posts/content-as-a-graph/,Displaying content as a graph: An exploration | This is important,"Most web content is displayed as a strict hierarchy, tree-based or otherwise. What if it wasn't?An exploration into the advantages of graph-based content design, the numerous pitfalls, and a few case studies along the way.",False,False
https://t.co/DWZ1nTPCwb,[2311.09677] R-Tuning: Teaching Large Language Models to Refuse Unknown Questions,"Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the knowledge gap between parametric knowledge and the instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate this new instruction tuning approach effectively improves a model's ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty during training displays a better ability to estimate uncertainty than uncertainty-based testing. Our code will be released at https://github.com/shizhediao/R-Tuning.",False,False
https://x.com/shizhediao/status/1741120400992465338?s=51,,,False,True
https://airgrid.io>,,,"(False, 'Exception occurred: HTTPSConnectionPool(host=\'airgrid.io%3e\', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(""<urllib3.connection.HTTPSConnection object at 0x16851f6d0>: Failed to resolve \'airgrid.io%3e\' ([Errno 8] nodename nor servname provided, or not known)""))')",False
https://memex.garden/,Memex,Get access to the long term memory of your chat groups.,False,False
https://rsdoiel.github.io/blog/2023/03/10/first-prototype-pse.html,first-prototype-pse,,False,False
https://github.com/jasonjmcghee/rem,GitHub - jasonjmcghee/rem: An open source approach to locally record and enable searching everything you view on your Apple Silicon.,An open source approach to locally record and enable searching everything you view on your Apple Silicon. - GitHub - jasonjmcghee/rem: An open source approach to locally record and enable searching...,False,False
https://www.getinboxzero.com,Inbox Zero,"Clean your inbox in minutes. Inbox Zero is the quickest way to reach inbox zero, with our newsletter cleaner, AI automation, and email analytics.",False,False
https://norvig.com/ngrams/,Natural Language Corpus Data: Beautiful Data,,False,False
https://blot.im/how,How to use Blot - Blot,,False,False
https://link.springer.com/content/pdf/10.1007/s12671-016-0594-9.pdf,,,True,False
https://www.patterns.app/,Patterns Analytics,Generated by the time you've found your dashboard,False,False
https://github.com/Frooodle/Stirling-PDF,GitHub - Stirling-Tools/Stirling-PDF: locally hosted web application that allows you to perform various operations on PDF files,locally hosted web application that allows you to perform various operations on PDF files - GitHub - Stirling-Tools/Stirling-PDF: locally hosted web application that allows you to perform various o...,False,False
https://news.ycombinator.com/item?id=38759877,Ask HN: How do I train a custom LLM/ChatGPT on my own documents in Dec 2023?,"There is a 5 month old thread [1] on this, but it might be already outdated.<p>What is the best approach for feeding custom set of documents to LLM and get non-halucinating and decent result in Dec 2023?<p>UPD: The question is generally about how to &quot;teach&quot; LLM answer questions using your set of documents (not necessarily train your own, so approaches like RAG counts)<p>[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36832572",False,False
http://bactra.org/notebooks/nn-attention-and-transformers.html,"""Attention"", ""Transformers"", in Neural Network ""Large Language Models""",,False,False
https://x.com/omarsar0/status/1738265651188253051?s=51,,,False,True
https://t.co/criuHgH5E8,[2312.10997v1] Retrieval-Augmented Generation for Large Language Models: A Survey,"Large language models (LLMs) demonstrate powerful capabilities, but they still face challenges in practical applications, such as hallucinations, slow knowledge updates, and lack of transparency in answers. Retrieval-Augmented Generation (RAG) refers to the retrieval of relevant information from external knowledge bases before answering questions with LLMs. RAG has been demonstrated to significantly enhance answer accuracy, reduce model hallucination, particularly for knowledge-intensive tasks. By citing sources, users can verify the accuracy of answers and increase trust in model outputs. It also facilitates knowledge updates and the introduction of domain-specific knowledge. RAG effectively combines the parameterized knowledge of LLMs with non-parameterized external knowledge bases, making it one of the most important methods for implementing large language models. This paper outlines the development paradigms of RAG in the era of LLMs, summarizing three paradigms: Naive RAG, Advanced RAG, and Modular RAG. It then provides a summary and organization of the three main components of RAG: retriever, generator, and augmentation methods, along with key technologies in each component. Furthermore, it discusses how to evaluate the effectiveness of RAG models, introducing two evaluation methods for RAG, emphasizing key metrics and abilities for evaluation, and presenting the latest automatic evaluation framework. Finally, potential future research directions are introduced from three aspects: vertical optimization, horizontal scalability, and the technical stack and ecosystem of RAG.",False,False
https://x.com/omarsar0/status/1738354427759612222?s=51,,,False,True
https://t.co/h7dsZ07Wbb,NeurIPS 2023 Recap ‚Äî Best Papers - Latent Space,"Our selection for AI Engineers: Word2Vec (with Jeff Dean), State Space Models (with Chris R√©), Emergence Mirage, DPO, Datablations, QLora, LlaVA, DataComp, Tree of Thought, CogEval, Voyager",False,False
https://x.com/latentspacepod/status/1738709627829883346?s=51,,,False,True
https://github.com/apple/ml-ferret,GitHub - apple/ml-ferret,Contribute to apple/ml-ferret development by creating an account on GitHub.,False,False
https://lea.verou.me/blog/2023/eigensolutions/?latest,Eigensolutions: composability as the antidote to overfit ‚Ä¢ Lea Verou,,False,False
https://nlpnewsletter.substack.com/p/nlp-research-in-the-era-of-llms,NLP Research in the Era of LLMs - by Sebastian Ruder,5 Key Research Directions Without Much Compute,False,False
https://news.ycombinator.com/item?id=38730143,Show HN: Emu2 ‚Äì A Gemini-like open-source 37B Multimodal Model,"Hello HN,
I&#x27;m excited to introduce Emu2, the latest generative multimodal model developed by the Beijing Academy of Artificial Intelligence (BAAI). Emu2 is an open-source initiative that reflects BAAI&#x27;s commitment to fostering open, secure, and responsible AI research. It&#x27;s designed to enhance AI&#x27;s proficiency in handling tasks across various modalities with minimal examples and straightforward instructions.<p>Emu2 has demonstrated superior performance over other large-scale models like Flamingo-80B in few-shot multimodal understanding tasks. It serves as a versatile base model for developers, providing a flexible platform for crafting specialized multimodal applications.<p>Key features of Emu2 include:<p>- A more streamlined modeling framework than its predecessor, Emu.<p>- A decoder capable of reconstructing images from the encoder&#x27;s semantic space.<p>- An expansion to 37 billion parameters, boosting both capabilities and generalization.<p>BAAI has also released fine-tuned versions, Emu2-Chat for visual understanding and Emu2-Gen for visual generation, which stand as some of the most powerful open-source models available today.<p>Here are the resources for those interested in exploring or contributing to Emu2:<p>- Project: https:&#x2F;&#x2F;baaivision.github.io&#x2F;emu2&#x2F;<p>- Model: https:&#x2F;&#x2F;huggingface.co&#x2F;BAAI&#x2F;Emu2<p>- Code: https:&#x2F;&#x2F;github.com&#x2F;baaivision&#x2F;Emu&#x2F;tree&#x2F;main&#x2F;Emu2<p>- Demo: https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;BAAI&#x2F;Emu2<p>- Paper: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.13286<p>We&#x27;re eager to see how the HN community engages with Emu2 and we welcome your feedback to help us improve. Let&#x27;s collaborate to push the boundaries of multimodal AI!",False,False
https://github.com/casibase/casibase,"GitHub - casibase/casibase: ‚ö°Ô∏èOpen-source LangChain-like AI knowledge database with web UI and Enterprise SSO‚ö°Ô∏è, supports OpenAI, Azure, Google Gemini, HuggingFace, OpenRouter, ChatGLM and local models, chat demo: https://ai.casbin.com, admin portal demo: https://ai.casibase.com","‚ö°Ô∏èOpen-source LangChain-like AI knowledge database with web UI and Enterprise SSO‚ö°Ô∏è, supports OpenAI, Azure, Google Gemini, HuggingFace, OpenRouter, ChatGLM and local models, chat demo: https://ai....",False,False
https://opendal.apache.org/docs/overview/,Welcome to Apache OpenDAL‚Ñ¢ | Apache OpenDAL‚Ñ¢,OpenDAL represents Open Data Access Layer. Our vision is to access data freely.,False,False
https://github.com/penxio/penx,GitHub - penxio/penx: A structured note-taking app for personal use.,A structured note-taking app for personal use. Contribute to penxio/penx development by creating an account on GitHub.,False,False
https://mindustrygame.github.io/,Mindustry,A sandbox tower-defense game.,False,False
https://wasmer.io/posts/introducing-the-wasmer-js-sdk,Introducing the new Wasmer JS SDK,"Today we are incredibly excited to present `@wasmer/sdk`, a new library that allows running WASI(X) applications easily on the browser",False,False
https://eu.getdot.ai/share/c80139c9-13f4-4db4-88f6-6e058ba31ad4?org_id=getdot.ai,Chat with Dot,,False,False
https://github.com/johnma2006/mamba-minimal,"GitHub - johnma2006/mamba-minimal: Simple, minimal implementation of the Mamba SSM in one file of PyTorch.","Simple, minimal implementation of the Mamba SSM in one file of PyTorch. - GitHub - johnma2006/mamba-minimal: Simple, minimal implementation of the Mamba SSM in one file of PyTorch.",False,False
https://www.plasmic.app/,Plasmic | Build powerful apps fast‚Äî without the limits,"Plasmic lets you build powerful web apps super fast, from internal tools to user facing products. Integrate with your codebase for unlimited possibilities.",False,False
https://github.com/S-LoRA/S-LoRA,GitHub - S-LoRA/S-LoRA: S-LoRA: Serving Thousands of Concurrent LoRA Adapters,S-LoRA: Serving Thousands of Concurrent LoRA Adapters - GitHub - S-LoRA/S-LoRA: S-LoRA: Serving Thousands of Concurrent LoRA Adapters,False,False
https://olano.dev/2023-12-12-reclaiming-the-web-with-a-personal-reader/,Reclaiming the Web with a Personal Reader,"There‚Äôs a kind of zen flow that programmers unblock when they experience their software daily as an end user. There‚Äôs no better catalyst for ideas and experimentation, no better prioritization driver than having to face the bugs, annoyances, and limitations of an application first-hand.",False,False
https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/,FunSearch: Making new discoveries in mathematical sciences using Large Language Models - Google DeepMind,"We introduce FunSearch, a method for searching for ‚Äúfunctions‚Äù written in computer code, and find new solutions in mathematics and computer science. FunSearch works by pairing a pre-trained LLM, whose goal is to provide creative solutions in the form of computer code, with an automated ‚Äúevaluator‚Äù, which guards against hallucinations and incorrect ideas.",False,False
https://www.answer.ai/posts/2023-12-12-launch.html,Answer.AI - A new old kind of R&D lab,Answer.AI is a new kind of AI R&D lab which creates practical end-user products based on foundational research breakthroughs.,False,False
https://github.com/elfvingralf/macOSpilot-ai-assistant,"GitHub - elfvingralf/macOSpilot-ai-assistant: Voice + Vision powered AI assistant that answers questions about any application, in context and in audio.","Voice + Vision powered AI assistant that answers questions about any application, in context and in audio. - GitHub - elfvingralf/macOSpilot-ai-assistant: Voice + Vision powered AI assistant that a...",False,False
https://future.mozilla.org/blog/introducing-memorycache/,"Introducing MemoryCache | Augmented Local AI
            
            
            
            - Mozilla Innovations",,False,False
https://github.com/SecureAI-Tools/SecureAI-Tools,GitHub - SecureAI-Tools/SecureAI-Tools: Private and secure AI tools for everyone's productivity.,Private and secure AI tools for everyone's productivity. - GitHub - SecureAI-Tools/SecureAI-Tools: Private and secure AI tools for everyone's productivity.,False,False
https://github.com/chriskiehl/Gooey,GitHub - chriskiehl/Gooey: Turn (almost) any Python command line program into a full GUI application with one line,Turn (almost) any Python command line program into a full GUI application with one line - GitHub - chriskiehl/Gooey: Turn (almost) any Python command line program into a full GUI application with o...,False,False
https://stylexjs.com/,StyleX,,False,False
https://www.dedp.online/,About this Book - üìñ Data Engineering Design Patterns (DEDP),,False,False
https://x.com/langchainai/status/1732447629374877699?s=51,,,False,True
https://arxiv.org/pdf/2312.02120.pdf,,,True,False
https://t.co/i8lfNwzrdO,Code GPT: Chat & AI Agents - Visual Studio Marketplace,Extension for Visual Studio Code - Easily Connect to Top AI Providers Using Their Official APIs in VSCode,False,False
https://t.co/0e80bEGxh8,Ollama,"Get up and running with large language models, locally.",False,False
https://x.com/dani_avila7/status/1731344666824523882?s=51,,,False,True
https://pytorch.org/blog/accelerating-generative-ai-2/,"Accelerating Generative AI with PyTorch II: GPT, Fast | PyTorch","This post is the second part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. We are excited to share a breadth of newly released PyTorch performance features alongside practical examples to see how far we can push PyTorch native performance. In part one, we showed how to accelerate Segment Anything over 8x using only pure, native PyTorch. In this blog we‚Äôll focus on LLM optimization.",False,False
https://x.com/sophiamyang/status/1730108858889097710?s=51,,,False,True
https://github.com/Avaiga/taipy,GitHub - Avaiga/taipy: Turns Data and AI algorithms into production-ready web applications in no time.,Turns Data and AI algorithms into production-ready web applications in no time. - GitHub - Avaiga/taipy: Turns Data and AI algorithms into production-ready web applications in no time.,False,False
https://github.com/Mozilla-Ocho/llamafile,GitHub - Mozilla-Ocho/llamafile: Distribute and run LLMs with a single file.,Distribute and run LLMs with a single file. Contribute to Mozilla-Ocho/llamafile development by creating an account on GitHub.,False,False
https://starling.cs.berkeley.edu/,Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF,,False,False
https://www.cam.ac.uk/research/news/ai-system-self-organises-to-develop-features-of-brains-of-complex-organisms,AI system self-organises to develop features of brains of complex organisms | University of Cambridge,Cambridge scientists have shown that placing physical constraints on an artificially-intelligent system ‚Äì in much the same way that the human brain has to,False,False
https://www.interconnects.ai/p/q-star,"The Q* hypothesis: Tree-of-thoughts reasoning, process reward models, and supercharging synthetic data","Emergency special: The information we need to understand what Q* is was right in front of us, but the memes are more fun than reality.",False,False
https://intelpython.github.io/DPEP/main/,Data Parallel Extensions for Python ‚Äî Data Parallel Extensions for Python* 0.1 documentation,,False,False
https://x.com/cwolferesearch/status/1727728638458540492?s=51,,,False,True
https://shuffle.dev/library-creator,UI Generator,A unique approach to Generative UIs where you have complete control.,False,False
https://github.com/THUDM/GLM,GitHub - THUDM/GLM: GLM (General Language Model),GLM (General Language Model). Contribute to THUDM/GLM development by creating an account on GitHub.,False,False
https://x.com/trending_repos/status/1726177938054472126?s=12,,,False,True
https://x.com/langchainai/status/1725175442708038047?s=51,,,False,True
https://robertrode.com/2023/11/15/operating-an-entire-company-on-a-minimal-two-core-postgresql-instance-query-optimization-insights-part-1.html,"Operating an Entire Company on a Minimal Two-Core PostgreSQL Instance: Query Optimization Insights, Part 1","In an era where the costs for cloud services continue to soar, I want to outline how our data team has successfully managed to run the infrastructure of our energy trading company within the first year using only two PostgreSQL database instances - each equipped with just two CPU cores and four GB of RAM. Despite this minimal use of resources, internal business processes and analytics dashboards ...",False,False
https://linear.app/blog/planning-for-unplanned-work,Planning for unplanned work,,False,False
https://www-cs-faculty.stanford.edu/~knuth/cweb.html,Knuth and Levy: CWEB,,False,False
https://flower.dev/blog/2023-11-15-federated-finetuning-of-openai-whisper-with-flower/,Federated Finetuning of OpenAI's Whisper,Check out the new code example federating OpenAI's Whisper for the downstream task of keyword spotting.,False,False
https://www.nature.com/articles/d41586-022-02161-5,Why thinking hard makes us feel tired,"Difficult tasks can lead to build-up of a signalling molecule in the brain, triggering fatigue. Difficult tasks can lead to build-up of a signalling molecule in the brain, triggering fatigue.",False,False
http://imlab.postech.ac.kr/dkim/class/csed514_2019s/DeepLearningBook.pdf,,,True,False
https://allenai.github.io/lumos/,"ü™Ñ Lumos: Language Agents with Unified Data Formats, Modular Design, and Open-Source LLMs",,False,False
https://www.tng-project.org/about/,IllustrisTNG - Project Description,The IllustrisTNG project. The next generation of cosmological hydrodynamical simulations of galaxy formation and evolution.,False,False
https://jxnl.github.io/instructor/blog/2023/11/05/chain-of-density/,Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density - Instructor,Enhancing OpenAI function calling with Pydantic,False,False
https://stanforddatascience.github.io/best-practices/index.html,"Open, rigorous and reproducible research: A practitioner‚Äôs handbook","Even though many scientists are expected to abide by the principles of open, rigorous and reproducible research, few scientists actually receive formal training in this. Indeed, there is no collection written by workings scientis for working scientists about the principles of open, rigorous and transparent research and how to go about practicing such research, beyond theory. This handbook represents a constantly updated guide aiming to fulfil this gap and by doing so actively assist all researchers in producing informative research.",False,False
https://journal.hexmos.com/gpu-survival-toolkit/,GPU Survival Toolkit for the AI age: The bare minimum every developer must know,"Why CPU Knowledge Is No Longer Enough



In today's AI age, the majority of developers train in the CPU way. This knowledge has been part of our academics as well, so it's obvious to think and problem-solve in a CPU-oriented way.



However, the problem with CPUs is that they rely",False,False
https://www.youtube.com/watch?v=rBpZvMAim5E,"LlamaIndex Webinar: Learn about Fine-tuning + RAG (w/ Victoria Lin, author of RA-DIT) - YouTube","We're excited to talk about how to combine fine-tuning and RAG with Victoria Lin, author of the recent RA-DIT (Retrieval-Augmented Dual Instruction Tuning) p...",False,False
https://medium.com/@yu-joshua/adding-structure-aware-retrieval-to-genai-stack-373976de14d6,Just a moment...,,False,False
https://x.com/langchainai/status/1721605114639892493?s=12,,,False,True
https://t.co/YlWEDBN1Mu,Paper page - CogVLM: Visual Expert for Pretrained Language Models,Join the discussion on this paper page,False,False
https://x.com/_akhaliq/status/1721758951396524259?s=51,,,False,True
https://www.fermyon.com/blog/introducing-spin-v2,Introducing Spin 2.0,"Spin 2.0 ‚Äî the open source developer tool for building, distributing, and running WebAssembly (Wasm) applications in the cloud.",False,False
https://medium.com/@neum_ai/retrieval-augmented-generation-at-scale-building-a-distributed-system-for-synchronizing-and-eaa29162521,Just a moment...,,False,False
http://clarifai.com/facebook/nougat/models/nougat-base,nougat-base model by facebook | Clarifai - The World's AI,"Nougat is a Meta AI-developed visual transformer model that converts document images, including complex math equations, into structured text, offering",False,False
https://t.co/SHsWnoWrBK?ssr=true,GitHub - outlines-dev/outlines: Guided Text Generation,Guided Text Generation. Contribute to outlines-dev/outlines development by creating an account on GitHub.,False,False
https://nerdyarticles.com/a-clutter-free-life-with-paperless-ngx/,A Clutter-Free Life: Going Paperless with Paperless-ngx,"Explore Paperless-ngx, the game-changer in document management. I'll take you through my adoption journey, Docker setup, and why it's 100 times better than paper filing. Say goodbye to paper chaos!",False,False
https://github.com/trholding/llama2.c,GitHub - trholding/llama2.c: Llama 2 Everywhere (L2E),Llama 2 Everywhere (L2E). Contribute to trholding/llama2.c development by creating an account on GitHub.,False,False
https://github.com/stanfordnlp/dspy,GitHub - stanfordnlp/dspy: Stanford DSPy: The framework for programming‚Äînot prompting‚Äîfoundation models,Stanford DSPy: The framework for programming‚Äînot prompting‚Äîfoundation models - GitHub - stanfordnlp/dspy: Stanford DSPy: The framework for programming‚Äînot prompting‚Äîfoundation models,False,False
https://arxiv.org/abs/2309.07315,[2309.07315] Traveling Words: A Geometric Interpretation of Transformers,"Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajectory of word particles along the hyper-sphere.",False,False
https://blog.reyem.dev/post/extracting_hn_book_recommendations_with_chatgpt_api/,Extracting Hacker News Book Recommendations with the ChatGPT API - reyem.dev blog,,False,False
https://www.lepton.ai/,Build AI The Simple Way | Lepton AI,"Run AI applications efficiently, at scale, and in minutes with a cloud native platform.",False,False
https://www.evidentlyai.com/ml-observability-course,Evidently AI - Open-source ML observability course,Free Open-source ML observability course for data scientists and ML engineers! Learn production ML monitoring at your own pace or sign up for the next cohort.,False,False
https://arxiv.org/abs/2310.01889,[2310.01889] Ring Attention with Blockwise Transformers for Near-Infinite Context,"Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.",False,False
https://jakelazaroff.com/words/an-interactive-intro-to-crdts/,An Interactive Intro to CRDTs | jakelazaroff.com,CRDTs don't have to be all academic papers and math jargon. Learn what CRDTs are and how they work through interactive visualizations and code samples.,False,False
https://blog.vespa.ai/vespa-is-becoming-its-own-company/,Vespa is becoming a company | Vespa Blog,Vespa is becoming its own company!,False,False
https://georgek.github.io/blog/posts/multiple-web-projects-traefik/,gpk blog - Working on Multiple Web Projects with Docker Compose and Traefik,,False,False
